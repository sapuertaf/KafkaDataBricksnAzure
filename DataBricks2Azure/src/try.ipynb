{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables_configs \n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "from pyspark.sql.functions import regexp_replace, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_schema = StructType()\\\n",
    "               .add(\"SalesOrderLineKey\",IntegerType())\\\n",
    "               .add(\"ResellerKey\",IntegerType())\\\n",
    "               .add(\"CustomerKey\",IntegerType())\\\n",
    "               .add(\"ProductKey\",IntegerType())\\\n",
    "               .add(\"OrderDateKey\",IntegerType())\\\n",
    "               .add(\"DueDateKey\",IntegerType())\\\n",
    "               .add(\"ShipDateKey\",IntegerType())\\\n",
    "               .add(\"SalesTerritoryKey\",IntegerType())\\\n",
    "               .add(\"OrderQuantity\",IntegerType())\\\n",
    "               .add(\"UnitPrice\",StringType())\\\n",
    "               .add(\"ExtendedAmount\",StringType())\\\n",
    "               .add(\"UnitPriceDiscountPct\",StringType())\\\n",
    "               .add(\"ProductStandardCost\",StringType())\\\n",
    "               .add(\"TotalProductCost\",StringType())\\\n",
    "               .add(\"SalesAmount\",StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = spark.read\\\n",
    "                .csv(\"C:\\\\Users\\\\Admin\\\\Desktop\\\\KafkaPipeline\\\\ETLocal2Kafka\\\\data\\\\sales.csv\", header = True, inferSchema = False, schema = sales_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_2_transform(table_name:str, columns_2_transform_type:str) -> list:\n",
    "    \"\"\"Obtener de las configuaraciones globales de la tabla (ConfigQuind) las columnas\n",
    "    con tipos de datos especiales a imputar. \n",
    "\n",
    "    Args:\n",
    "        table_name (str): Nombre de la tabla que posee un tipo de dato especifico a inputar\n",
    "        columns_2_transform_type (str): Tipo de dato especifico a imputar\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tuplas que contiene el nombre de la columns y su tipo especificado\n",
    "              en las configuarciones globales.\n",
    "\n",
    "    Example:\n",
    "    >>> get_columns_2_transform(\"sales\",\"price\")\n",
    "            [('ExtendedAmount', 'price'),\n",
    "             ('UnitPrice', 'price'),\n",
    "             ('TotalProductCost', 'price'),\n",
    "             ('SalesAmount', 'price')]\n",
    "    >>> get_columns_2_transform(\"date\",\"price\")\n",
    "            []\n",
    "    \"\"\"\n",
    "    # me aseguro de que la tabla tenga configurada variables globales\n",
    "    try:\n",
    "       tables_configs.global_confs[table_name]\n",
    "    except KeyError:\n",
    "       # en caso de que la tabla no tenga configuraciones especificadas retornar lista vacia\n",
    "       return []\n",
    "    assert columns_2_transform_type in tables_configs.columns_types_2_transform, f\"{columns_2_transform_type} type not in tables_config allowed types\"\n",
    "    return list(filter(lambda x:x[1] == columns_2_transform_type,tables_configs.global_confs[table_name].items()))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def transform_prices_2_numeric(df:SparkDataFrame, table_name:str) -> SparkDataFrame:\n",
    "    \"\"\"Imputar datos de las columnas de la tabla 'table_name' que tienen datos de tipo 'price' \n",
    "    a datos flotantes. \n",
    "\n",
    "    Args:\n",
    "        df (SparkDataFrame): DataFrame de Spark que contiene los datos leidos del topico de Kafka.\n",
    "        table_name (str): Nombre de la tabla que contiene las columnas con tipos de datos\n",
    "        'price' a imputar.\n",
    "\n",
    "    Returns:\n",
    "        SparkDataFrame: DataFrame de Spark con columnas que contiene tipos de datos 'price' imputadas.\n",
    "    \n",
    "    Example:\n",
    "    >>> df_input.show()\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "        |SalesOrderLineKey|ResellerKey|CustomerKey|ProductKey|OrderDateKey|DueDateKey|ShipDateKey|SalesTerritoryKey|OrderQuantity|UnitPrice|ExtendedAmount|UnitPriceDiscountPct|ProductStandardCost|TotalProductCost|SalesAmount|\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "        |         43659001|        676|         -1|       349|    20170702|  20170712|   20170709|                5|            1|$2.024,99|     $2.024,99|               0,00%|          $1.898,09|       $1.898,09|  $2.024,99|\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "    \n",
    "    >>> transform_prices_2_numeric(df_iput,\"sales\")\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "        |SalesOrderLineKey|ResellerKey|CustomerKey|ProductKey|OrderDateKey|DueDateKey|ShipDateKey|SalesTerritoryKey|OrderQuantity|UnitPrice|ExtendedAmount|UnitPriceDiscountPct|ProductStandardCost|TotalProductCost|SalesAmount|\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "        |         43659001|        676|         -1|       349|    20170702|  20170712|   20170709|                5|            1|  2024.99|       2024.99|               0,00%|            1898.09|         1898.09|    2024.99|\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "    \"\"\"\n",
    "    if len(get_columns_2_transform(table_name,\"price\")) == 0:\n",
    "       return df\n",
    "    \n",
    "    columns_2_transform = list(map(\n",
    "        lambda x: x[0],\n",
    "        get_columns_2_transform(table_name,\"price\")\n",
    "    ))\n",
    "    # checkeo que las columnas retornadas si esten en el dataframe\n",
    "    assert len(set(columns_2_transform) - set(df.columns)) == 0, f\"Mismatch between df columns and global {table_name} columns\" \n",
    "    for column in columns_2_transform:\n",
    "        df = df.withColumn(\n",
    "            column,\n",
    "            regexp_replace(\n",
    "                regexp_replace(col(column), \"[$.]\", \"\"), \"[,]\",\".\"\n",
    "            ).alias(column).cast(\"float\")\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def transform_percentages_2_numeric(df:SparkDataFrame, table_name:str) -> SparkDataFrame:\n",
    "    \"\"\"Imputar datos de las columnas de la tabla 'table_name' que tienen datos de tipo 'percentage' \n",
    "    a datos flotantes.  \n",
    "\n",
    "\n",
    "    Args:\n",
    "        df (SparkDataFrame): DataFrame de Spark que contiene los datos leidos del topico de Kafka.\n",
    "        table_name (str): Nombre de la tabla que contiene las columnas con tipos de datos\n",
    "        'percentage' a imputar.\n",
    "\n",
    "    Returns:\n",
    "        SparkDataFrame: DataFrame de Spark con columnas que contiene tipos de datos 'percentage' imputadas.\n",
    "\n",
    "    Example:\n",
    "    >>> df_input.show()\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "        |SalesOrderLineKey|ResellerKey|CustomerKey|ProductKey|OrderDateKey|DueDateKey|ShipDateKey|SalesTerritoryKey|OrderQuantity|UnitPrice|ExtendedAmount|UnitPriceDiscountPct|ProductStandardCost|TotalProductCost|SalesAmount|\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "        |         43659001|        676|         -1|       349|    20170702|  20170712|   20170709|                5|            1|$2.024,99|     $2.024,99|               0,00%|          $1.898,09|       $1.898,09|  $2.024,99|\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "    \n",
    "    >>> transform_percentages_2_numeric(df_input, \"sales\")\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "        |SalesOrderLineKey|ResellerKey|CustomerKey|ProductKey|OrderDateKey|DueDateKey|ShipDateKey|SalesTerritoryKey|OrderQuantity|UnitPrice|ExtendedAmount|UnitPriceDiscountPct|ProductStandardCost|TotalProductCost|SalesAmount|\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "        |         43659001|        676|         -1|       349|    20170702|  20170712|   20170709|                5|            1|$2.024,99|     $2.024,99|                 0.0|          $1.898,09|       $1.898,09|  $2.024,99|\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "    \"\"\"\n",
    "    if len(get_columns_2_transform(table_name,\"percentage\")) == 0:\n",
    "       return df\n",
    "    \n",
    "    columns_2_transform = list(map(\n",
    "        lambda x: x[0],\n",
    "        get_columns_2_transform(table_name,\"percentage\")\n",
    "    ))\n",
    "    # checkeo que las columnas retornadas si esten en el dataframe\n",
    "    assert len(set(columns_2_transform) - set(df.columns)) == 0, f\"Mismatch between df columns and global {table_name} columns\"\n",
    "    for column in columns_2_transform:\n",
    "        df = df.withColumn(\n",
    "            column,\n",
    "            regexp_replace(\n",
    "                regexp_replace(col(column), \"[%]\", \"\"), \"[,]\",\".\"\n",
    "            ).alias(column).cast(\"float\")\n",
    "        )\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "def main(df_input:SparkDataFrame, table_name:str):\n",
    "    df_prices_transformed = transform_prices_2_numeric(df_input,table_name)\n",
    "    df_percentages_transformed = transform_percentages_2_numeric(df_prices_transformed,table_name)\n",
    "    return df_percentages_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = spark.read\\\n",
    "                .csv(\"C:\\\\Users\\\\puert\\\\OneDrive\\\\Escritorio\\\\KafkaDataBricksnAzure\\\\ETLocal2Kafka\\\\data\\\\sales.csv\", header = True, inferSchema = False, schema = sales_schema)\n",
    "\n",
    "clean_sales= main(sales_df, \"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
      "|SalesOrderLineKey|ResellerKey|CustomerKey|ProductKey|OrderDateKey|DueDateKey|ShipDateKey|SalesTerritoryKey|OrderQuantity|UnitPrice|ExtendedAmount|UnitPriceDiscountPct|ProductStandardCost|TotalProductCost|SalesAmount|\n",
      "+-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
      "|         43659001|        676|         -1|       349|    20170702|  20170712|   20170709|                5|            1|$2.024,99|     $2.024,99|               0,00%|          $1.898,09|       $1.898,09|  $2.024,99|\n",
      "+-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
      "|SalesOrderLineKey|ResellerKey|CustomerKey|ProductKey|OrderDateKey|DueDateKey|ShipDateKey|SalesTerritoryKey|OrderQuantity|UnitPrice|ExtendedAmount|UnitPriceDiscountPct|ProductStandardCost|TotalProductCost|SalesAmount|\n",
      "+-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
      "|         43659001|        676|         -1|       349|    20170702|  20170712|   20170709|                5|            1|  2024.99|       2024.99|                 0.0|            1898.09|         1898.09|    2024.99|\n",
      "+-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_sales.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = spark.read\\\n",
    "                .csv(\"C:\\\\Users\\\\puert\\\\OneDrive\\\\Escritorio\\\\KafkaDataBricksnAzure\\\\ETLocal2Kafka\\\\data\\\\date.csv\", header = True, inferSchema = False, schema = schema_date)\n",
    "clean_date = main(date_df, \"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+-------------+--------+------------+--------+\n",
      "| DateKey|      Date|FiscalYear|FiscalQuarter|   Month|    FullDate|MonthKey|\n",
      "+--------+----------+----------+-------------+--------+------------+--------+\n",
      "|20170701|01/07/2017|    FY2018|    FY2018 Q1|2017 Jul|2017 Jul, 01|  201707|\n",
      "+--------+----------+----------+-------------+--------+------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+-------------+--------+------------+--------+\n",
      "| DateKey|      Date|FiscalYear|FiscalQuarter|   Month|    FullDate|MonthKey|\n",
      "+--------+----------+----------+-------------+--------+------------+--------+\n",
      "|20170701|01/07/2017|    FY2018|    FY2018 Q1|2017 Jul|2017 Jul, 01|  201707|\n",
      "+--------+----------+----------+-------------+--------+------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_date.show(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
