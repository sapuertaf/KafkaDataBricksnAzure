{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables_configs\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql.types import StructType, IntegerType,StringType,FloatType,DecimalType\n",
    "from pyspark.sql.functions import from_json, col, regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_ACCOUNT_NAME = \"azuredatalakespuertaf\"\n",
    "RAW_PATH = f\"wasbs://raw@{STORAGE_ACCOUNT_NAME}.blob.core.windows.net\"\n",
    "MOUNT_POINT_RAW = \"/mnt/raw/\"\n",
    "RAW_CHECKPOINT_LOCATION = \"/mnt/raw/{}/check/\"\n",
    "DATABRICKS_SECRET_SCOPE = \"accessdatalake\"\n",
    "AZURE_SECRET_PATH = f\"fs.azure.account.key.{STORAGE_ACCOUNT_NAME}.blob.core.windows.net\"\n",
    "AZURE_SECRET_NAME = \"datalakeaccess\"\n",
    "ENTITIES = [\"customer\",\"date\",\"product\",\"reseller\",\"sales\",\"sales_order\",\"sales_territory\"]\n",
    "TOPICS = [\"prueba.customer\",\"prueba.date\",\"prueba.product\",\"prueba.reseller\",\"prueba.sales\",\"prueba.sales-order\",\"prueba.sales-territory\"]\n",
    "KAFKA_SERVER = \"20.49.8.224:9092\"\n",
    "SAVE_DATA_FORMAT = \"delta\"\n",
    "SAVE_DATA_MODE = \"append\"\n",
    "OUT_PATHS = {entity:MOUNT_POINT_RAW+entity+\"/\" for entity in ENTITIES}\n",
    "print(OUT_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configurando y montando puntos de montaje raw DataBricks - Azure\n",
    "dbutils.fs.mount(\n",
    "    source = RAW_PATH,\n",
    "    mount_point = MOUNT_POINT_RAW,\n",
    "    extra_configs = {AZURE_SECRET_PATH:dbutils.secrets.get(scope=DATABRICKS_SECRET_SCOPE, key=AZURE_SECRET_NAME)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_customer = StructType()\\\n",
    "                  .add(\"CustomerKey\",IntegerType())\\\n",
    "                  .add(\"CustomerID\",StringType())\\\n",
    "                  .add(\"Customer\",StringType())\\\n",
    "                  .add(\"City\",StringType())\\\n",
    "                  .add(\"StateProvince\",StringType())\\\n",
    "                  .add(\"CountryRegion\",StringType())\\\n",
    "                  .add(\"PostalCode\",StringType())\n",
    "\n",
    "\n",
    "schema_date = StructType()\\\n",
    "              .add(\"DateKey\",IntegerType())\\\n",
    "              .add(\"Date\",StringType())\\\n",
    "              .add(\"FiscalYear\",StringType())\\\n",
    "              .add(\"FiscalQuarter\",StringType())\\\n",
    "              .add(\"Month\",StringType())\\\n",
    "              .add(\"FullDate\",StringType())\\\n",
    "              .add(\"MonthKey\",IntegerType())\n",
    "\n",
    "schema_product = StructType()\\\n",
    "                 .add(\"ProductKey\",IntegerType())\\\n",
    "                 .add(\"SKU\",StringType())\\\n",
    "                 .add(\"Product\",StringType())\\\n",
    "                 .add(\"StandardCost\",FloatType())\\\n",
    "                 .add(\"Color\",StringType())\\\n",
    "                 .add(\"ListPrice\",FloatType())\\\n",
    "                 .add(\"Model\",StringType())\\\n",
    "                 .add(\"Subcategory\",StringType())\\\n",
    "                 .add(\"Category\",StringType())\n",
    "\n",
    "schema_reseller = StructType()\\\n",
    "                 .add(\"ResellerKey\",IntegerType())\\\n",
    "                 .add(\"ResellerID\",StringType())\\\n",
    "                 .add(\"BusinessType\",StringType())\\\n",
    "                 .add(\"Reseller\",StringType())\\\n",
    "                 .add(\"City\",StringType())\\\n",
    "                 .add(\"StateProvince\",StringType())\\\n",
    "                 .add(\"CountryRegion\",StringType())\\\n",
    "                 .add(\"PostalCode\",StringType())\n",
    "\n",
    "schema_sales = StructType()\\\n",
    "               .add(\"SalesOrderLineKey\",IntegerType())\\\n",
    "               .add(\"ResellerKey\",IntegerType())\\\n",
    "               .add(\"CustomerKey\",IntegerType())\\\n",
    "               .add(\"ProductKey\",IntegerType())\\\n",
    "               .add(\"OrderDateKey\",IntegerType())\\\n",
    "               .add(\"DueDateKey\",IntegerType())\\\n",
    "               .add(\"ShipDateKey\",IntegerType())\\\n",
    "               .add(\"SalesTerritoryKey\",IntegerType())\\\n",
    "               .add(\"OrderQuantity\",IntegerType())\\\n",
    "               .add(\"UnitPrice\",FloatType())\\\n",
    "               .add(\"ExtendedAmount\",FloatType())\\\n",
    "               .add(\"UnitPriceDiscountPct\",DecimalType(5,2))\\\n",
    "               .add(\"ProductStandardCost\",FloatType())\\\n",
    "               .add(\"TotalProductCost\",FloatType())\\\n",
    "               .add(\"SalesAmount\",FloatType())\n",
    "\n",
    "schema_sales_order = StructType()\\\n",
    "                 .add(\"Channel\",StringType())\\\n",
    "                 .add(\"SalesOrderLineKey\",IntegerType())\\\n",
    "                 .add(\"SalesOrder\",StringType())\\\n",
    "                 .add(\"SalesOrderLine\",IntegerType())\n",
    "\n",
    "schema_sales_territory = StructType()\\\n",
    "                         .add(\"SalesTerritoryKey\",IntegerType())\\\n",
    "                         .add(\"Region\",StringType())\\\n",
    "                         .add(\"Country\",StringType())\\\n",
    "                         .add(\"Group\",StringType())\n",
    "                         \n",
    "schemas = [schema_customer,schema_date,schema_product,schema_reseller,schema_sales,schema_sales_order,schema_sales_territory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_kafka_topic(kafka_server:str, topic_name:str) -> SparkDataFrame:\n",
    "     \"\"\"Leer topico de kafka del servidor y convertirlo a streaming de datos. \n",
    "\n",
    "     Args:\n",
    "         kafka_server (str): Direccion IP y puerto para acceder a la maquina virtual con kafka. \n",
    "         topic_name (str): Nombre del topico de kafka del cual se extraeran los datos.\n",
    "\n",
    "     Returns:\n",
    "         SparkDataFrame: Streamin de datos del topico \n",
    "     \"\"\"\n",
    "     stream_df = spark.readStream\\\n",
    "         .format(\"kafka\")\\\n",
    "         .option(\"kafka.bootstrap.servers\", kafka_server)\\\n",
    "         .option(\"subscribe\",topic_name)\\\n",
    "         .option(\"startingOffsets\",\"earliest\")\\\n",
    "         .option(\"failOnDataLoss\",\"false\")\\\n",
    "         .load()\n",
    "     return stream_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(stream_df:SparkDataFrame, schema:StructType) -> SparkDataFrame:\n",
    "    \"\"\"Se obtiene los datos base del topico aplicandole el esquema provisto.\n",
    "\n",
    "    Args: \n",
    "            df(SparkDataFrame): DataFrame de Spark base del topico.\n",
    "            Schema(StruckType): Eschema de datos esperados en la trama.\n",
    "    Returns: \n",
    "            df(SparkDataFrame): DataFrame de Spark con los datos de la trama parseados segÃºn el esquema definido.\n",
    "  \"\"\"\n",
    "    df = stream_df.select(from_json(col(\"value\").cast(StringType()),schema).alias(\"data\"))\n",
    "    df = df.select(\"data.*\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_columns_2_transform(table_name:str, columns_2_transform_type:str) -> list:\n",
    "    \"\"\"Obtener de las configuaraciones globales de la tabla (ConfigQuind) las columnas\n",
    "    con tipos de datos especiales a imputar. \n",
    "\n",
    "    Args:\n",
    "        table_name (str): Nombre de la tabla que posee un tipo de dato especifico a inputar\n",
    "        columns_2_transform_type (str): Tipo de dato especifico a imputar\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tuplas que contiene el nombre de la columns y su tipo especificado\n",
    "              en las configuarciones globales.\n",
    "\n",
    "    Example:\n",
    "    >>> get_columns_2_transform(\"sales\",\"price\")\n",
    "            [('ExtendedAmount', 'price'),\n",
    "             ('UnitPrice', 'price'),\n",
    "             ('TotalProductCost', 'price'),\n",
    "             ('SalesAmount', 'price')]\n",
    "    >>> get_columns_2_transform(\"date\",\"price\")\n",
    "            []\n",
    "    \"\"\"\n",
    "    # me aseguro de que la tabla tenga configurada variables globales\n",
    "    try:\n",
    "        tables_configs.global_confs[table_name]\n",
    "    except KeyError:\n",
    "        #en caso de que la tabla no tenga configuraciones especificadas se retorna lista vacia\n",
    "        return []\n",
    "    assert columns_2_transform_type in tables_configs.columns_types_2_transform, f\"{columns_2_transform_type} type not in tables_config allowed types\"\n",
    "    return list(filter(lambda x:x[1] == columns_2_transform_type,tables_configs.global_confs[table_name].items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_prices_2_numeric(df:SparkDataFrame, table_name:str) -> SparkDataFrame:\n",
    "    \"\"\"Imputar datos de las columnas de la tabla 'table_name' que tienen datos de tipo 'price' \n",
    "    a datos flotantes. \n",
    "\n",
    "    Args:\n",
    "        df (SparkDataFrame): DataFrame de Spark que contiene los datos leidos del topico de Kafka.\n",
    "        table_name (str): Nombre de la tabla que contiene las columnas con tipos de datos\n",
    "        'price' a imputar.\n",
    "\n",
    "    Returns:\n",
    "        SparkDataFrame: DataFrame de Spark con columnas que contiene tipos de datos 'price' imputadas.\n",
    "    \n",
    "    Example:\n",
    "    >>> df_input.show()\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "        |SalesOrderLineKey|ResellerKey|CustomerKey|ProductKey|OrderDateKey|DueDateKey|ShipDateKey|SalesTerritoryKey|OrderQuantity|UnitPrice|ExtendedAmount|UnitPriceDiscountPct|ProductStandardCost|TotalProductCost|SalesAmount|\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "        |         43659001|        676|         -1|       349|    20170702|  20170712|   20170709|                5|            1|$2.024,99|     $2.024,99|               0,00%|          $1.898,09|       $1.898,09|  $2.024,99|\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "    \n",
    "    >>> transform_prices_2_numeric(df_iput,\"sales\")\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "        |SalesOrderLineKey|ResellerKey|CustomerKey|ProductKey|OrderDateKey|DueDateKey|ShipDateKey|SalesTerritoryKey|OrderQuantity|UnitPrice|ExtendedAmount|UnitPriceDiscountPct|ProductStandardCost|TotalProductCost|SalesAmount|\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "        |         43659001|        676|         -1|       349|    20170702|  20170712|   20170709|                5|            1|  2024.99|       2024.99|               0,00%|            1898.09|         1898.09|    2024.99|\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "    \"\"\"\n",
    "    if len(get_columns_2_transform(table_name,\"price\")) == 0:\n",
    "        return df\n",
    "    \n",
    "    columns_2_transform = list(map(\n",
    "        lambda x: x[0],\n",
    "        get_columns_2_transform(table_name,\"price\")\n",
    "    ))\n",
    "    # checkeo que las columnas retornadas si esten en el dataframe\n",
    "    assert len(set(columns_2_transform) - set(df.columns)) == 0, f\"Mismatch between df columns and global {table_name} columns\" \n",
    "    for column in columns_2_transform:\n",
    "        df = df.withColumn(\n",
    "            column,\n",
    "            regexp_replace(\n",
    "                regexp_replace(col(column), \"[$.]\", \"\"), \"[,]\",\".\"\n",
    "            ).alias(column).cast(\"float\")\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_percentages_2_numeric(df, table_name):\n",
    "    \"\"\"Imputar datos de las columnas de la tabla 'table_name' que tienen datos de tipo 'percentage' \n",
    "    a datos flotantes.  \n",
    "\n",
    "\n",
    "    Args:\n",
    "        df (SparkDataFrame): DataFrame de Spark que contiene los datos leidos del topico de Kafka.\n",
    "        table_name (str): Nombre de la tabla que contiene las columnas con tipos de datos\n",
    "        'percentage' a imputar.\n",
    "\n",
    "    Returns:\n",
    "        SparkDataFrame: DataFrame de Spark con columnas que contiene tipos de datos 'percentage' imputadas.\n",
    "\n",
    "    Example:\n",
    "    >>> df_input.show()\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "        |SalesOrderLineKey|ResellerKey|CustomerKey|ProductKey|OrderDateKey|DueDateKey|ShipDateKey|SalesTerritoryKey|OrderQuantity|UnitPrice|ExtendedAmount|UnitPriceDiscountPct|ProductStandardCost|TotalProductCost|SalesAmount|\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "        |         43659001|        676|         -1|       349|    20170702|  20170712|   20170709|                5|            1|$2.024,99|     $2.024,99|               0,00%|          $1.898,09|       $1.898,09|  $2.024,99|\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "    \n",
    "    >>> transform_percentages_2_numeric(df_input, \"sales\")\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "        |SalesOrderLineKey|ResellerKey|CustomerKey|ProductKey|OrderDateKey|DueDateKey|ShipDateKey|SalesTerritoryKey|OrderQuantity|UnitPrice|ExtendedAmount|UnitPriceDiscountPct|ProductStandardCost|TotalProductCost|SalesAmount|\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "        |         43659001|        676|         -1|       349|    20170702|  20170712|   20170709|                5|            1|$2.024,99|     $2.024,99|                 0.0|          $1.898,09|       $1.898,09|  $2.024,99|\n",
    "        +-----------------+-----------+-----------+----------+------------+----------+-----------+-----------------+-------------+---------+--------------+--------------------+-------------------+----------------+-----------+\n",
    "    \"\"\"\n",
    "    if len(get_columns_2_transform(table_name,\"percentage\")) == 0:\n",
    "        return df\n",
    "    \n",
    "    columns_2_transform = list(map(\n",
    "        lambda x: x[0],\n",
    "        get_columns_2_transform(table_name,\"percentage\")\n",
    "    ))\n",
    "    # checkeo que las columnas retornadas si esten en el dataframe\n",
    "    assert len(set(columns_2_transform) - set(df.columns)) == 0, f\"Mismatch between df columns and global {table_name} columns\"\n",
    "    for column in columns_2_transform:\n",
    "        df = df.withColumn(\n",
    "            column,\n",
    "            regexp_replace(\n",
    "                regexp_replace(col(column), \"[%]\", \"\"), \"[,]\",\".\"\n",
    "            ).alias(column).cast(\"float\")\n",
    "        )\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data_df:SparkDataFrame,format_:str,out_path:str,mode:str, check_path:str):\n",
    "    \"\"\"\n",
    "    Funcion principal que define todo el proceso de streaming: carga la trama base, genera la funcion de procesamiento y las rutas de checkpoint. \n",
    "    Args: \n",
    "        df(SparkDataFrame): DataFrame de Spark\n",
    "        format(str): Formato de Almacenamiento: delta, parquet, etc\n",
    "        output_path(str): Ruta de almacenamiento\n",
    "        query_name(str): Nombre del Query \n",
    "        mode(str): Modo de almacenamiento\n",
    "        check_path(str): Ruta de Almacenamiento del checkpoint\n",
    "        \n",
    "    Returns \n",
    "        Streaming \n",
    "  \"\"\"\n",
    "    return data_df.writeStream\\\n",
    "                  .format(format_)\\\n",
    "                  .option(\"path\",out_path)\\\n",
    "                  .outputMode(mode)\\\n",
    "                  .option(\"checkpointLocation\",check_path)\\\n",
    "                  .option(\"mergeSchema\",\"true\")\\\n",
    "                  .trigger(processingTime=\"30 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(table_name:str, kafka_server:str, topic_name:str, schema:StructType,format_:str, out_path:str,mode:str, check_path:str):\n",
    "    \"\"\"\n",
    "    Funcion principal que define todo el proceso de streaming: carga la trama base, la parsea, ejecuta la logica de negocio y escribe los datos resultado. \n",
    "    Args: \n",
    "        table_name(str): Nombre de la tabla \n",
    "        kafka_server(str): Direccion IP del Broker del servicio de Kafka.\n",
    "        topic_name(str): Topico de kafka \n",
    "        Schema(StruckType): Eschema de datos esperados en la trama de entrada.\n",
    "        format_(str): Formato de Almacenamiento: delta, parquet, etc\n",
    "        out_path(str): Ruta de almacenamiento\n",
    "        mode(str): Modo de almacenamiento\n",
    "        check_path(str): Ruta de Almacenamiento del checkpoint\n",
    "    Returns \n",
    "        Streaming \n",
    "  \"\"\"\n",
    "    kafka_data = read_kafka_topic(\n",
    "        kafka_server=kafka_server,\n",
    "        topic_name=topic_name\n",
    "    )\n",
    "    parsed_data = get_data(\n",
    "        stream_df=kafka_data,\n",
    "        schema=schema\n",
    "    )\n",
    "    \n",
    "    clean_prices_df = transform_prices_2_numeric(\n",
    "        df = parsed_data,\n",
    "        table_name=table_name,\n",
    "    )\n",
    "    clean_percentages_df = transform_percentages_2_numeric(\n",
    "        df=clean_prices_df,\n",
    "        table_name=table_name,\n",
    "    )\n",
    "    saved_data = save_data(\n",
    "        data_df=clean_percentages_df,\n",
    "        format_=format_,\n",
    "        out_path=out_path,\n",
    "        mode = mode,\n",
    "        check_path=check_path\n",
    "    )\n",
    "    return saved_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = main(\n",
    "        table_name = \"customer\",\n",
    "        kafka_server= KAFKA_SERVER,\n",
    "        topic_name=\"prueba.customer\",\n",
    "        schema = schema_customer,\n",
    "        format_= SAVE_DATA_FORMAT,\n",
    "        out_path= OUT_PATHS[\"customer\"],\n",
    "        mode = SAVE_DATA_MODE,\n",
    "        check_path=RAW_CHECKPOINT_LOCATION.format(\"customer\")\n",
    "    )\n",
    "customer_df.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = main(\n",
    "        table_name=\"date\",\n",
    "        kafka_server= KAFKA_SERVER,\n",
    "        topic_name=\"prueba.date\",\n",
    "        schema = schema_customer,\n",
    "        format_= SAVE_DATA_FORMAT,\n",
    "        out_path= OUT_PATHS[\"date\"],\n",
    "        mode = SAVE_DATA_MODE,\n",
    "        check_path=RAW_CHECKPOINT_LOCATION.format(\"date\")\n",
    "    )\n",
    "date_df.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = main(\n",
    "        table_name=\"sales\",\n",
    "        kafka_server= KAFKA_SERVER,\n",
    "        topic_name=\"prueba.sales\",\n",
    "        schema = schema_customer,\n",
    "        format_= SAVE_DATA_FORMAT,\n",
    "        out_path= OUT_PATHS[\"sales\"],\n",
    "        mode = SAVE_DATA_MODE,\n",
    "        check_path=RAW_CHECKPOINT_LOCATION.format(\"sales\")\n",
    "    )\n",
    "sales_df.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para detener los procesos de streaming\n",
    "for s in spark.streams.active:\n",
    "      s.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
