{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql.types import StructType, IntegerType,StringType,FloatType,DecimalType\n",
    "from pyspark.sql.functions import from_json, col, expr, base64\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_ACCOUNT_NAME = \"azuredatalakespuertaf\"\n",
    "RAW_PATH = f\"wasbs://raw@{STORAGE_ACCOUNT_NAME}.blob.core.windows.net\"\n",
    "MOUNT_POINT_RAW = \"/mnt/raw/{}\"\n",
    "RAW_CHECKPOINT_LOCATION = \"/mnt/raw/{}/check\"\n",
    "DATABRICKS_SECRET_SCOPE = \"accessdatalake\"\n",
    "AZURE_SECRET_PATH = f\"fs.azure.account.key.{STORAGE_ACCOUNT_NAME}.blob.core.windows.net\"\n",
    "AZURE_SECRET_NAME = \"datalakeaccess\"\n",
    "ENTITIES = [\"customer\",\"date\",\"product\",\"reseller\",\"sales\",\"sales_order\",\"sales_territory\"]\n",
    "TOPICS = [\"prueba.customer\",\"prueba.date\",\"prueba.product\",\"prueba.reseller\",\"prueba.sales\",\"prueba.sales-order\",\"prueba.sales-terrritory\"]\n",
    "KAFKA_SERVER = \"74.249.34.106:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configurando y montando puntos de montaje raw DataBricks - Azure\n",
    "def gen_mount_point(entiti_name:str):\n",
    "    dbutils.fs.mount(\n",
    "        source = RAW_PATH,\n",
    "        mount_point = MOUNT_POINT_RAW.format(entiti_name),\n",
    "        extra_configs = {AZURE_SECRET_PATH:dbutils.secrets.get(scope=DATABRICKS_SECRET_SCOPE, key=AZURE_SECRET_NAME)}\n",
    "    )\n",
    "\n",
    "progress_bar = tqdm(total=len(ENTITIES))\n",
    "for entiti in ENTITIES:\n",
    "    gen_mount_point(entiti)\n",
    "    progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_customer = StructType()\\\n",
    "                  .add(\"CustomerKey\",IntegerType())\\\n",
    "                  .add(\"CustomerID\",StringType())\\\n",
    "                  .add(\"Customer\",StringType())\\\n",
    "                  .add(\"City\",StringType())\\\n",
    "                  .add(\"StateProvince\",StringType())\\\n",
    "                  .add(\"CountryRegion\",StringType())\\\n",
    "                  .add(\"PostalCode\",StringType())\n",
    "\n",
    "\n",
    "schema_date = StructType()\\\n",
    "              .add(\"DateKey\",IntegerType())\\\n",
    "              .add(\"Date\",StringType())\\\n",
    "              .add(\"FiscalYear\",StringType())\\\n",
    "              .add(\"FiscalQuarter\",StringType())\\\n",
    "              .add(\"Month\",StringType())\\\n",
    "              .add(\"FullDate\",StringType())\\\n",
    "              .add(\"MonthKey\",IntegerType())\n",
    "\n",
    "schema_product = StructType()\\\n",
    "                 .add(\"ProductKey\",IntegerType())\\\n",
    "                 .add(\"SKU\",StringType())\\\n",
    "                 .add(\"Product\",StringType())\\\n",
    "                 .add(\"StandardCost\",FloatType())\\\n",
    "                 .add(\"Color\",StringType())\\\n",
    "                 .add(\"ListPrice\",FloatType())\\\n",
    "                 .add(\"Model\",StringType())\\\n",
    "                 .add(\"Subcategory\",StringType())\\\n",
    "                 .add(\"Category\",StringType())\n",
    "\n",
    "schema_reseller = StructType()\\\n",
    "                 .add(\"ResellerKey\",IntegerType())\\\n",
    "                 .add(\"ResellerID\",StringType())\\\n",
    "                 .add(\"BusinessType\",StringType())\\\n",
    "                 .add(\"Reseller\",StringType())\\\n",
    "                 .add(\"City\",StringType())\\\n",
    "                 .add(\"StateProvince\",StringType())\\\n",
    "                 .add(\"CountryRegion\",StringType())\\\n",
    "                 .add(\"PostalCode\",StringType())\n",
    "\n",
    "schema_sales = StructType()\\\n",
    "               .add(\"SalesOrderLineKey\",IntegerType())\\\n",
    "               .add(\"ResellerKey\",IntegerType())\\\n",
    "               .add(\"CustomerKey\",IntegerType())\\\n",
    "               .add(\"ProductKey\",IntegerType())\\\n",
    "               .add(\"OrderDateKey\",IntegerType())\\\n",
    "               .add(\"DueDateKey\",IntegerType())\\\n",
    "               .add(\"ShipDateKey\",IntegerType())\\\n",
    "               .add(\"SalesTerritoryKey\",IntegerType())\\\n",
    "               .add(\"OrderQuantity\",IntegerType())\\\n",
    "               .add(\"UnitPrice\",FloatType())\\\n",
    "               .add(\"ExtendedAmount\",FloatType())\\\n",
    "               .add(\"UnitPriceDiscountPct\",DecimalType(5,2))\\\n",
    "               .add(\"ProductStandardCost\",FloatType())\\\n",
    "               .add(\"TotalProductCost\",FloatType())\\\n",
    "               .add(\"SalesAmount\",FloatType())\n",
    "\n",
    "schema_sales_order = StructType()\\\n",
    "                 .add(\"Channel\",StringType())\\\n",
    "                 .add(\"SalesOrderLineKey\",IntegerType())\\\n",
    "                 .add(\"SalesOrder\",StringType())\\\n",
    "                 .add(\"SalesOrderLine\",IntegerType())\n",
    "\n",
    "schema_sales_territory = StructType()\\\n",
    "                         .add(\"SalesTerritoryKey\",IntegerType())\\\n",
    "                         .add(\"Region\",StringType())\\\n",
    "                         .add(\"Country\",StringType())\\\n",
    "                         .add(\"Group\",StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_kafka_topic(kafka_server:str, topic_name:str) -> SparkDataFrame:\n",
    "    stream_df = spark.readStream\\\n",
    "         .format(\"kafka\")\\\n",
    "         .option(\"kafka.bootstrap.servers\", kafka_server)\\\n",
    "         .option(\"subscribe\",topic_name)\\\n",
    "         .option(\"startingOffsets\",\"earliest\")\\\n",
    "         .option(\"failOnDataLoss\",\"false\")\\\n",
    "         .load()\n",
    "    return stream_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(stream_df:SparkDataFrame, schema:StructType) -> SparkDataFrame:\n",
    "    df = stream_df.select(from_json(col(\"value\").cast(StringType()),schema).alias(\"data\"))\n",
    "    df = df.select(\"data.*\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data_df:SparkDataFrame, format_:str, out_path:str,mode:str, check_path:str):\n",
    "    return data_df.writeStream\\\n",
    "                  .format(format_)\\\n",
    "                  .option(\"path\",out_path)\\\n",
    "                  .outputMode(mode)\\\n",
    "                  .option(\"checkpointLocation\",check_path)\\\n",
    "                  .trigger(processingTime=\"30 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(kafka_server:str, topic_name:str, schema:StructType,format_:str, out_path:str,mode:str, check_path:str):\n",
    "    kafka_data = read_kafka_topic(\n",
    "        kafka_server=kafka_server,\n",
    "        topic_name=topic_name\n",
    "    )\n",
    "    parsed_data = get_data(\n",
    "        stream_df=kafka_data,\n",
    "        schema=schema\n",
    "    )\n",
    "    saved_data = save_data(\n",
    "        data_df=parsed_data,\n",
    "        format_=format_,\n",
    "        out_path=out_path,\n",
    "        mode = mode,\n",
    "        check_path=check_path\n",
    "    )\n",
    "    return saved_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_kafka_topic(\n",
    "    kafka_server=KAFKA_SERVER,\n",
    "    topic_name=TOPICS[0],\n",
    ")\n",
    "\n",
    "parsed = get_data(\n",
    "    stream_df=data,\n",
    "    schema = schema_customer\n",
    ")\n",
    "\n",
    "display(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para detener los procesos de streaming\n",
    "for s in spark.streams.active:\n",
    "      s.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
